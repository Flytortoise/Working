第五课 机器是如何学习的
    计算机只能处理数值和运算

    学习：确定基本模型，输入数据，使用算法，逐步调整模型参数，最终让模型的结果满足大多数数据
    模型：分类模型，学习的结果
    算法：学习时的工具
    

    有监督学习：通过标注数据进行学习
    无监督学习：通过非标注数据进行学习



第六课 机器学习三要素之数据、模型、算法
    算法通过在数据上进行运算产生模型

    数据：
        向量空间模型：将输入数据（文字、图片、音频等），转化为一个能表现其特征的一维或多维向量
        无标注数据：向量没有被打上标签，机器需要自己判断
        有标注数据：向量被人为打上标签

        准备数据的步骤：
        1.确定特征工程：
            （1）确定用哪些特征来表示数据
            （2）确定用什么方式表达这些数据
        2.进行VSM转换，将数据变成有特征的向量


    模型：
        模型是机器学习的结果，学习的过程，称之为训练
        训练就是：根据已经被指定的 f(x) 的具体形式——模型类型，结合训练数据，计算出其中各个参数的具体取值的过程。
        训练过程需要依据某种章法进行运算。这个章法，就是算法。


    算法：
        有监督学习和无监督学习的算法差别比较大
        有监督学习的目标就是：让训练数据的所有 x 经过 f(x) 计算后，获得的 y’ 与它们原本对应的 y 的差别尽量小。
        损失函数：理论y1与实际y的差值函数，针对一个训练数据
        代价函数：描述整体损失，自变量为所有待定的参数
        学习目标：最小化代价函数，及使用优化算法，让代价函数降低


    要得到高质量的模型，算法、数据都很重要


第七课 模型的获取和改进
    训练：获取模型的过程
        Step-1：数据准备。
            Step-1.1 数据预处理：收集数据、清洗数据、标注数据。
            Step-1.2 构建数据的向量空间模型（将文本、图片、音频、视频等格式的数据转换为向量）。
            Step-1.3 将构建好向量空间模型的数据分为训练集、验证集和测试集。每个集合应当是独立的，和另外两个没有重叠。
        Step-2：训练——将训练集输入给训练程序，进行运算。训练程序的核心是算法，所有输入的向量化数据都会按该训练程序所依据的算法进行运算。训练程序输出的结果，就是模型。
            Step-2.1： 编写训练程序。
                Step-2.1.1： 选择模型类型；
                Step-2.1.2： 选择优化算法；
                Step-2.1.3： 根据模型类型和算法编写程序。
            Step-2.2： 训练 -> 获得临时模型。
            Step-2.3： 在训练集上运行临时模型，获得训练集预测结果。
            Step-2.4： 在验证集上运行临时模型，获得验证集预测结果。
            Step-2.5： 综合参照 Step-2.4 和 Step-2.5 的预测结果，改进模型。
                改进的三方面：
                （1）数据：大量的高质量训练数据，是提高模型质量的最有效手段
                          对于有限数据来说，透过以下三种方式来提高数据质量：
                          （Ⅰ）对数据进行归一化、正则化等标准化操做
                          （Ⅱ）采用Bootstrap等采样方法处理有限训练/测试数据
                          （Ⅲ）根据业务进行特征选取：从业务角度区分输入数据包含的特征，并理解这些特征对结果的影响
                （2）算法：调参，找到合适的超参数（需要手动设置和调整）
                （3）模型：选择最适合的模型
            Step-2.6： Step-2.2 到 Step-2.5 反复迭代，直至获得让我们满意，或者已经无法继续优化的模型。
        Step-3：测试——将测试集数据输入给训练获得的模型，得到预测结果；再将预测结果与这些数据原本预期的结果进行比较。




第八课 模型的质量和评判指标
    分类模型评判指标：Precision、Recall和F1Score

    TP：实际为 Class_A,预测也为 Class_A
    FP：实际为 Class_A,预测不为 Class_A
    FN：实际不为 Class_A,预测为 Class_A
    精准率：Precision=TP/（TP+FP），即在所有被预测为 Class_A 的测试数据中，预测正确的比率。
    召回率：Recall=TP/（TP+FN），即在所有实际为 Class_A 的测试数据中，预测正确的比率。
    F1Score = 2*(Precision * Recall)/(Precision + Recall)

    这三个指标越大越好，但P和R往往是矛盾的。
    P,R,F1Score在分类问题中都是对某一个类而言，同时指向一个模型和一个数据集
    衡量一个模型的质量，要看所有的指标，而不是一套


    欠拟合：一般在训练集上的预测结果就不佳，指标偏低
    过拟合：在训练集上指标很好，而在验证/测试集上指标偏低
    解决方方法：
        欠拟合多数情况下是因为选定模型类型太过简单，特征选取不够导致的。
        而过拟合则相反，可能是模型太过复杂，特征选择不当（过多或组合不当）造成的。
        有针对性地选择更复杂/简单的模型类型；增加/减少特征；或者减小/增大正则项比重等。



第九课 最常用的优化算法————梯度下降法
    机器学习目标：最小化目标函数（代价函数）
    常要学习的几个经典机器学习模型的目标函数，都是凸函数

    凸函数：
        某个向量空间的凸子集（区间）上的实值函数，如果在其定义域上的任意两点 ，
        有 f(tx + (1-t)y) <= tf(x) + (1-t)f(y)，则称其为该区间上的凸函数。

    函数存在多个极值时，我们要找到定义域内的最小值，而不是极小值


    梯度下降法：（寻找最小值的一种常用方法）
        1. 随机取一个自变量的值 x0
        2. 对应该自变量算出对应点的因变量值：f(x0)
        3. 计算 f(x0) 处目标函数 f(x) 的导数
        4. 从 f(x0) 开始，沿着该处目标函数导数的方向，按一个指定的步长α，向前“走一步”，走到的位置对应自变量取值为 x1。
        换言之，| x0 – x1| /α = f(x) 在 f（x0）处的斜率；
        5. 继续重复2-4，直至退出迭代（达到指定迭代次数，或 f(x) 近似收敛到最优解）

        即选区合适的步长，一步步到达最小值；在这里步长就是超参数，需要自行设置，调整
        步长过大，可能会错过最小值，步长过小，那么迭代的次数就过多，运算时间，也就越长


    梯度下降算法的难点：
        当出现多个极小值时，可能第一次遇到极小值之后，就结束了，但此时不是最小值。
        如果目标函数不能确定只有一个极小值，而获得的模型结果又不令人满意时，
        就该考虑是否是在学习的过程中，优化算法进入了局部而非全局最小值。
        这种情况下，可以尝试几个不同的起始点。甚至尝试一下大步长，说不定反而能够跨出局部最小值点所在的凸域。

